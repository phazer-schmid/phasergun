# API Server Environment Variables

# Anthropic API Key
# Get your API key from: https://console.anthropic.com/
# This is required for real LLM analysis
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Server Configuration
PORT=3001

# LLM Configuration
# Options: "anthropic" (cloud API - costs money), "ollama" (local/free), or "mock" (testing)
LLM_MODE=mock

# Anthropic Configuration (for LLM_MODE=anthropic)
# Options: claude-3-opus-20240229, claude-3-haiku-20240307
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Ollama Configuration (for LLM_MODE=ollama)
# Recommended models:
# - llama3.1:70b (best quality for FDA compliance, needs 48GB RAM)
# - llama3.1:8b (faster, lower quality, needs 8GB RAM)
# - qwen2.5:72b (alternative to llama, needs 48GB RAM)
# - mixtral:8x7b (good balance, needs 24GB RAM)
OLLAMA_MODEL=llama3.1:70b
OLLAMA_BASE_URL=http://localhost:11434
