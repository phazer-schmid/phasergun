# API Server Environment Variables

# Anthropic API Key
# Get your API key from: https://console.anthropic.com/
# This is required for real LLM analysis
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Server Configuration
PORT=3001

# LLM Configuration
# Options: "anthropic" (cloud API - costs money), "ollama" (local/free), or "mock" (testing)
LLM_MODE=mock

# Anthropic Configuration (for LLM_MODE=anthropic)
# Cost Optimized Setup (Recommended):
# - claude-3-haiku-20240307: ULTRA COST-EFFECTIVE! $0.25/$1.25 per 1M tokens (input/output)
#   Perfect for generating critical recommendations. 98% cost savings vs Sonnet.
# 
# High Quality Setup (For complex analysis):
# - claude-3-5-sonnet-20241022: Premium quality. $3/$15 per 1M tokens (input/output)
#   Use only when you need detailed reasoning and explanations.
#
# Current setup optimized for: Critical recommendations only (max 500 tokens output)
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Ollama Configuration (for LLM_MODE=ollama)
# Recommended models:
# - llama3.1:70b (best quality for FDA compliance, needs 48GB RAM)
# - llama3.1:8b (faster, lower quality, needs 8GB RAM)
# - qwen2.5:72b (alternative to llama, needs 48GB RAM)
# - mixtral:8x7b (good balance, needs 24GB RAM)
OLLAMA_MODEL=llama3.1:70b
OLLAMA_BASE_URL=http://localhost:11434
